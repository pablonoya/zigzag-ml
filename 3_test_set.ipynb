{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿Cómo probamos un modelo?\n",
    "La mejor manera de probar un modelo es utilizarlo el mundo real, mandarlo a enfrentar la vida prediciendo nuevos datos que no haya aprendido.\n",
    "\n",
    "¿Pero cómo sabemos si está listo?\n",
    "\n",
    "Reservaremos un conjunto que no verá durante el entrenamiento, el **test set** o conjunto de prueba. Por supuesto, el otro conjunto será el **training set** o conjunto de entrenamiento.\n",
    "\n",
    "![training_test_set](https://data-flair.training/blogs/wp-content/uploads/sites/2/2018/08/1-16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con frecuencia, se divide el dataset en 80% para training y 20% para test. Valores de 70-30 o 60-40 también son comunes.  \n",
    "Pero **antes debemos mezclarlo** como una baraja de naipes, ¿por qué? necesitamos una muestra aleatoria que represente a todo el conjunto.\n",
    "\n",
    "Imaginemos que los datos fueron tomados por zonas y en orden, una parte, en el orden original, podrían ser las viviendas de una zona específica, esa parte podría faltar en el entrenamiento y nuestro modelo no aprenderá de ella.\n",
    "\n",
    "Si tomamos datos aleatorios, tendremos una muestra que represente en **general** a todo el dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primero mezclamos\n",
    "Sea n el tamaño del dataset.  \n",
    "Definiremos una [semilla](https://es.wikipedia.org/wiki/Semilla_aleatoria). que los resultados sean [reproducibles](https://es.wikipedia.org/wiki/Reproducibilidad_y_repetibilidad), esto es, siempre los mismos.  \n",
    "Creamos un array de 0 a n-1 que representará los **indices**, lo desordenamos al azar y reemplazamos **X** e **y** con los **indices desordenados**, es importante que sean los **mismos indices**, cada **X** tiene su correspondiente **y**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# semilla\n",
    "np.random.seed(42)\n",
    "\n",
    "#probemos usar\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "# ¿cuántos datos tenemos?\n",
    "n = len(X)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array de 0 a n-1\n",
    "indices = np.arange(n)\n",
    "\n",
    "print(indices[:5], \"...\", indices[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desordenamos\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "print(indices[:5], \"...\")\n",
    "\n",
    "# reemplazamos\n",
    "X = X[indices]\n",
    "y = y[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ahora dividimos\n",
    "Antes debemos seleccionar usar alguna de [sus features](https://scikit-learn.org/stable/datasets/index.html#boston-house-prices-dataset), probemos con la última.  \n",
    "Dividamos en 60-40, necesitamos el indice que marque dónde termina el 60%  \n",
    "Tanto **train set** como **test set** son términos que agrupan ambas variables X e y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todas las filas, última columna\n",
    "X_selected = X[:, -1]\n",
    "\n",
    "# el índice debe ser entero\n",
    "training_size = int(n * 0.6)\n",
    "\n",
    "# seleccionamos el primer 60%\n",
    "X_train = X_selected[:training_size]\n",
    "y_train = y[:training_size]\n",
    "\n",
    "# y el l 40% restante\n",
    "X_test = X_selected[training_size:]\n",
    "y_test = y[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.scatter(X_test, y_test)\n",
    "plt.xlabel(\"LSTAT\")\n",
    "plt.ylabel(\"Valor\")\n",
    "plt.title(\"Conjuntos de entrenamiento y prueba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambos conjuntos son representativos, ¿verdad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probando, probando\n",
    "Primero entrenamos **sólo en el train set**.  \n",
    "\n",
    "Luego predecimos y calculamos el **MAE** usando los datos no vistos, el **test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train.reshape(-1, 1), y_train)\n",
    "\n",
    "y_hat = model.predict(X_test.reshape(-1, 1))\n",
    "print(f\"MAE (test set) {mean_absolute_error(y_hat, y_test) :.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta cantidad es más fiable, como el modelo no conoce estos datos, que además son reales, podemos decir que  nuestro modelo ha enfrentado la vida real :D\n",
    "\n",
    "Pero mientras mejoramos el modelo calculamos el error del train set, anímate a **c_mpletar el código**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos los valores predecidos usando el train set\n",
    "y_hat_train = model.___(X___.reshape(-1, 1))\n",
    "\n",
    "# calcularmos el MAE del train set\n",
    "print(f\"MAE (train set): {___error(___, y_tr___) :.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mayoría de las veces, el error del train set será menor, porque son los datos que \"mejor conoce\" el modelo, píensa que mientras más sepas de algo, mejor habrás aprendido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión polinomial\n",
    "Mira la gráfica, notarás que una recta no es suficiente para ajustar bien los datos, hace falta una **curva** cuya ecuación es un **polinomio**:\n",
    "\n",
    "$$P(x) = a_nx^n + a_{n-1}x^{n-1} + \\cdots + a_0x^0$$\n",
    "\n",
    "Notarás que se parece a la regresión lineal múltiple, con una pequeña gran diferencia, se usa la **misma feature elevada a un exponente**, esto se conoce como regresión **polinomial**, y se ajusta como si fuera una regresión lineal múltiple.\n",
    "\n",
    "Probaremos elevando a dos, incluyendo la misma feature al cuadrado el modelo pensará que son features diferentes :D  \n",
    "Podemos \"apilar\" varios arrays contenidos en una tupla utilizando `column_stack` de numpy, esto tratará cada array como una columna y es necesario que todos los arrays tengan la misma longitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly_train = np.column_stack((X_train ** 2, X_train))\n",
    "X_poly_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenando el modelo, de seguro tendremos un error menor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_poly = LinearRegression()\n",
    "model_poly.fit(X_poly_train, y_train)\n",
    "\n",
    "y_hat_poly = model_poly.predict(X_poly_train)\n",
    "print(f\"MAE: {mean_absolute_error(y_hat_poly, y_train) :.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veamos la curva\n",
    "Si unimos sólo 2 puntos tendremos una recta aunque usemos la ecuación polinómica de `model_poly`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpoints = np.array([min(X_train), max(X_train)])\n",
    "xpoints_poly = np.column_stack((xpoints ** 2, xpoints))\n",
    "\n",
    "ypoints = model_poly.predict(xpoints_poly)\n",
    "\n",
    "plt.plot(xpoints, ypoints, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos valores intermedios, mientras más tengamos, más suave será nuestra curva. La función `linspace` de numpy nos permite generar n valores equidistantes entre sí en un rango definido.\n",
    "\n",
    "Generemos 5 valores, el rango será el mínimo y el máximo de X_train, que contiene los valores originales, sin elevar al cuadrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpoints = np.linspace(start=min(X_train), stop=max(X_train), num=5)\n",
    "xpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpoints_poly = np.column_stack((xpoints ** 2, xpoints))\n",
    "ypoints = model_poly.predict(xpoints_poly)\n",
    "\n",
    "plt.plot(xpoints, ypoints, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para suavizar la curva no es necesario generar **demasiados** valores.\n",
    "Veamos también el train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpoints = np.linspace(start=min(X_train), stop=max(X_train), num=15)\n",
    "xpoints_poly = np.column_stack((xpoints ** 2, xpoints))\n",
    "ypoints = model_poly.predict(xpoints_poly)\n",
    "\n",
    "plt.plot(xpoints, ypoints, color=\"red\")\n",
    "plt.scatter(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genera polinomios\n",
    "El objeto [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html?highlight=polynomial#sklearn-preprocessing-polynomialfeatures) de sklearn convierte las features en polinomios, el parámetro `degree` indica el [grado](https://es.wikipedia.org/wiki/Grado_(polinomio)), esto es, el exponente máximo al que elevaremos.\n",
    "\n",
    "Su método `fit_transform` nos devolverá una matriz con cada fila en este formato: $x^0, x^1, \\cdots, x^n$ si el parámetro es **una** sola feature. En realidad este método llama a otros dos `fit` para calcular qué features tendremos y `transform` para crear dichas featuras.\n",
    "\n",
    "Como $x^0 = 1$ tendremos una columna llena de unos, este término se conoce como **bias** o sesgo, por ahora lo excluiremos estableciendo `include_bias` en falso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_train.reshape(-1, 1))\n",
    "X_poly[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a necesitar el reshape a (-1, 1) porque podemos incluir varias features, en caso de hacerlo tendremos un **polinomio de varias variales**.  \n",
    "Veamos uno de dos variables y grado dos sin coeficientes:\n",
    "\n",
    "${x_1}^2 + x_1 + x_1x_2 + x_2 +{x_2}^2$\n",
    "\n",
    "El exponente máximo es dos, aunque el término del medio tenga las dos variables, la suma de sus exponentes resulta en 2 pues ambos tienen grado 1.  \n",
    "De modo que un término con la forma ${x_1}^2x_2$ tendría grado 3.\n",
    "\n",
    "En nuestro caso de dos variables y grado 2 `PolynomialFeatures` devolverá una lista con el formato $x_1, x_2, {x_1}^2, x_1x_2, {x_2}^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambiamos de grado\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# tomamos dos features\n",
    "X_selected = X[:, [5, 7]]\n",
    "\n",
    "# recuerda que sólo tomamos una parte del dataset\n",
    "X_train = X_selected[:training_size]\n",
    "\n",
    "X_poly_train = poly.fit_transform(X_train)\n",
    "X_poly_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¡Pero no generes tantos!\n",
    "Prueba aumentar el grado, verás más columnas, y muchas más si añades features antes de generar polinomios.\n",
    "\n",
    "Como mencioné, estas columnas cuentan como nuevas features para el modelo, nuevas features pueden disminuir el error, pero por cada columna **añades una dimensión** y esto causa un problema conocido como **curse of dimensionality** o maldición de la dimensión.\n",
    "\n",
    "Esto trae \"problemas que no se ven en menores dimensiones\", como la **dispesión** y la **cercanía** de los datos.  \n",
    "Lo explicaré con un juego, estás en el principio y debes debes alcanzar un objeto que está al final en un espacio de 3 secciones:\n",
    "\n",
    "    OX*\n",
    "    \n",
    "Sólo debes recorrer 2 secciones para alcanzarlo, estás muy cerca del objeto, lo alcanzas y pasas de nivel.\n",
    "\n",
    "Ahora el juego tiene de un espacio de 3 secciones de largo y 3 de alto, siendo 9 en total:\n",
    "\n",
    "    XX*\n",
    "    XXX\n",
    "    OXX\n",
    "\n",
    "Sólo debes recorrer 4 secciones para alcanzar el objeto, ya no está tan cerca pero no tardas en alcanzarlo y pasar a otro nivel.\n",
    "\n",
    "Se añade otra dimensión también con 3 secciones, tendemos un juego 3D con 27 secciones y la misma situación.\n",
    "¿Cuántas secciones debes recorrer? \n",
    "\n",
    "Al añadir dimensiones, tú y el objeto se han **dispersado** perdiendo **cercanía**, lo mismo pasa con los datos cuando añadimos features, será más dificil para el modelo encontrar la recta que mejor se ajuste porque tenemos distancias más grandes.\n",
    "\n",
    "A menos que la dimensión extra nos brinde mejor información o tengamos un modelo más complejo, como una curva que pueda acercarse a los datos.  \n",
    "Esta excepción es importante, un modelo **más complejo** sólo será recomendable si tiene **más datos** de los que aprender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probando la curva\n",
    "Veamos si el modelo polinomial se desempeña mejor al final, para esto es necesario transformar también el test set, usaremos `transform` en este ya que tendrá las mismas features y no necesitamos volver a calcularlas.\n",
    "\n",
    "Siendo la sección final, **debes completarla** con lo visto en este capítulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuestra feature original\n",
    "X_selected = X[:, -1]\n",
    "\n",
    "# toma ambos conjuntos\n",
    "X_train = X_selected[:tr___siz_]\n",
    "X_test = X_selected[___:]\n",
    "\n",
    "# ¡transforma!\n",
    "poly = P___ialFeatures(degree=2, include_bias=___)\n",
    "\n",
    "X_poly_train = poly.fit_tr___(X_tr___.reshape(___))\n",
    "X_poly_test = poly.transform(X_tes___.r___(___))\n",
    "\n",
    "# entrena\n",
    "model_poly = ___Regr_ssion()\n",
    "model_poly.___(X_poly_train, y_tr___)\n",
    "\n",
    "# predice\n",
    "y_hat_train = model_poly.pr___(X_poly_train)\n",
    "y_hat_test = model_poly.___(X_poly_test)\n",
    "\n",
    "# calcula los errores para comparar\n",
    "print(f\"MAE (test set): {___error(y_h___, y_te__) :.2f}\")\n",
    "print(f\"MAE (training set): {___err___(y___, y___) :.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos una diferencia considerable entre ambos conjuntos, pero si ambos son representativos y ambos son una curva ¿qué pasó entonces? \n",
    "\n",
    "Un modelo más complejo permite aprender mejor, si aprendes mejor unas cuantas cosas... estarías perdiendo la idea general.  \n",
    "Lo mismo pasa con machine learning, es importante que lo aprendido se pueda **generalizar**, que funcione fuera de lo aprendido, de lo contrario tenemos un problema llamado **overfitting** o [sobreajuste](./4_ajuste)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
