{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af3df6f-fe01-49b2-b100-5fd97754b4ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mkdir -p datasets\n",
    "%cd datasets\n",
    "! wget -nc https://raw.githubusercontent.com/pablonoya/zigzag-ml/master/datasets/Iris.csv\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2ab46a-ded8-4c6a-8e0b-5cafcd4d9310",
   "metadata": {},
   "source": [
    "# Redes Neuronales Artificiales\n",
    "Quiero empezar con una cita del libro *Hands-On Machine Learning with Scikit-Learn, Keras and Tensorflow*\n",
    "> Los pájaros nos inspiraron para volar, la Bardana inspiró el Velcro, y la naturaleza ha inspirado innumerables inventos más. Parece lógico, pues, fijarnos en la arquitectura del cerebro para inspirarnos en la construcción de una máquina inteligente.\n",
    ">\n",
    "> Aurélien Géron, 2019, p. 310\n",
    "\n",
    "Estas redes artificiales son a menudo representadas por **capas de neuronas totalmente conectadas entre sí**, podemos distinguir tres partes:\n",
    "1. Capa de entrada\n",
    "2. Una o varias capas ocultas\n",
    "3. Capa de salida.\n",
    "\n",
    "![Red neuronal artificial](./img/10.1_Artificial_neural_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3e428d-0886-4dad-9265-1682e487dcaf",
   "metadata": {},
   "source": [
    "## Implementación con Keras\n",
    "Este tipo de redes son denominadas *fully connected* y pueden implementarse como un modelo `Sequential` con capas `Dense`, porque son una secuencia de capas densamente conectadas 😉.  \n",
    "Tomaremos ambos elementos del módulo `keras` de tensorflow 2, que tiene submódulos como `models` y `layers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f843abe-f6cf-4f3c-9577-a88e3e569328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo secuencial\n",
    "from tensorflow.keras.models import Sequential\n",
    "# capa densa\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997070e7-8f81-4c9d-bb22-eb6a7167b2aa",
   "metadata": {},
   "source": [
    "Podemos definir las capas del modelo pasándolas en una lista al momento de instanciarlo, cada capa `Dense` conecta las neuronas por nosotros, sólo debemos definir el número de neuronas en la primera, correspondiente a la capa de entrada, con el argumento `input_shape` el cual recibe una tupla, pero dejaremos la segunda dimensión vacía.  \n",
    "Para las capas posteriores, no es necesario definir este argumento, es calculado a partir de la capa anterior 😉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c177778-31e4-49f4-9d2b-28f46fa49a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(3, input_shape=(2,)),\n",
    "    Dense(2)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed54627-027d-40ae-9faa-62b841c4635d",
   "metadata": {},
   "source": [
    "Podemos ver un resumen de la arquitectura de nuestra red con el método `summary` este nos muestra el tipo de capas, las shapes de salida, donde None significa un número variable, y el número de parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05184dd-b11d-4e14-acff-5e88a2409239",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8530e66b-1187-42c8-8b7c-599d1d053035",
   "metadata": {},
   "source": [
    "También podemos instanciar nuestro modelo para añadir después las capas con el método `add`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc99829-f18c-41d6-b6a5-8e47c832a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(3, input_shape=(2,)))\n",
    "model2.add(Dense(2, input_shape=(2,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8687f6e7-9771-4bd0-a277-ce571686da97",
   "metadata": {},
   "source": [
    "Obteniendo el mismo resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f62385-1fa3-4c96-b455-411ab9b33048",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2cf189-10cd-4ef5-83b3-b0c7cb0b392d",
   "metadata": {},
   "source": [
    "# Neurona biológica, neurona artificial\n",
    "Las **neuronas artificiales** nacieron de la idea de **replicar las neuronas biológicas**, estas **reciben un impulso por medio de sus dendritas y lo transmiten a través de su axón** hacia otras neuronas, formando una red.\n",
    "\n",
    "![Neurona biológica](img/10.2.1_Biological_neuron.png)\n",
    "\n",
    "De la misma manera, una neurona artificial **recibe la información de otras neurona asignándoles pesos** o _weights_, realiza una **sumatoria de las entradas ponderadas**, y **pasa el resultado por una función de activación** para propagar una respuesta.\n",
    "\n",
    "![Neurona artificial](img/10.2.2_Artificial_neuron.png)\n",
    "\n",
    "¿Recuerdas el término independiente?, también conocido como *bias* y también presente en las redes neuronales cumple la misma función de dotar de mayor libertad al ajuste de parámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cd93eb-adbc-41b1-b61d-26bf38ba5c88",
   "metadata": {},
   "source": [
    "# Función de activación\n",
    "Cumple la tarea de romper la linealidad de las redes neuronales, podemos usar funciones como la sigmoide, pero la más utilizada es la función *Rectified Linear Unit* o **ReLU** para los amigos. Se define con una fórmula muy sencilla, la cual implementaremos.\n",
    "\n",
    "$$R(z) = max(0, z)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4bb6c-e7e0-4632-b01f-3f2b7f0ae337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(z):\n",
    "    return np.max((0, z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817cb6fa-5611-4d18-9fc6-9aefc3f27b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xpoints = np.linspace(-1, 1, 11)\n",
    "ypoints = [relu(x) for x in xpoints]\n",
    "\n",
    "plt.plot(xpoints, ypoints)\n",
    "plt.grid()\n",
    "plt.ylim(-0.2, 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6469c5-5858-4b3f-9ad7-4384c61960f6",
   "metadata": {},
   "source": [
    "**Esta función se aplica entre capas**, a excepción de la última, cuya función de activación cambiará dependiendo de nuestra tarea:\n",
    "\n",
    "- Función **linear**, equivalente a no usar ninguna función, si deseamos realizar **regresión**.\n",
    "- Función **sigmoide** si deseamos realizar **clasificación binaria**.\n",
    "- Función **softmax** para **clasificación múltiple**.\n",
    "\n",
    "Además el **número de neuronas en la capa de salida** dependerá de la tarea que busquemos resolver, siendo **sólo una para regresión o clasificación binaria** y el **número de clases** para la **clasificación múltiple**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c934b12-d3a2-47e6-a776-715b3aa238a1",
   "metadata": {},
   "source": [
    "# Clasificando Iris\n",
    "Veamos otra vez más nuestro dataset de Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2736bd1e-2223-4f03-a21d-c77a428dfd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./datasets/Iris.csv')\n",
    "data.drop('Id', axis='columns', inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d72656f-2a88-45af-a5be-55e3b6301c17",
   "metadata": {},
   "source": [
    "Ahora dividimos en features y labels, luego en conjuntos train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e100189-82e8-40c6-b59b-15af915b73e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop('Species', 'columns')\n",
    "\n",
    "# utilizaremos one-hot encoding\n",
    "y = pd.get_dummies(data['Species'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e477093-4c44-46bd-bf47-9dcfea0d8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79577742-2eaf-45ba-9ee8-f9c0099093f4",
   "metadata": {},
   "source": [
    "## Modelo: Red neuronal\n",
    "Tenemos 4 features, ese será el número de neuronas en nuestra capa de entrada.  \n",
    "Para las capas ocultas, usualmente se recomienda usar potencias de 2, además de seguir con la forma que vimos al principio lara que la red sea más grande en el centro, definamos 2 capas de 8 neuronas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de284524-f5a2-4be4-8309-6c4ba3930c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_iris = Sequential([\n",
    "    Dense(8, input_dim=4),\n",
    "    Dense(8, activation='relu'),\n",
    "    # capa de salida, activación softmax\n",
    "    Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20689981-f9c4-436e-8e8d-14efc1424f80",
   "metadata": {},
   "source": [
    "## Configuración\n",
    "Mediante el método `compile` podemos definir opciones adicionales, **la función de costo es obligatoria**, definida por el parámetro `loss` siendo `\"categorical_crossentropy\"` para el caso de softmax.  \n",
    "\n",
    "También cambiaremos el **optimizador** por el [Descenso del gradiente estocástico](Descenso del gradiente estocástico), el cual intercambia la precisión del gradiente por la velocidad, pues calcula el gradiente a partir de una muestra aleatoria de datos en lugar de utilizar todos, esta idea es bastante útil en datasets gigantes 🤯. Podemos establecerlo con el parámetro `optimizer=\"sgd\"`.  \n",
    "\n",
    "Además podemos incluir métricas adicionales, el costo se nos mostrará por defecto, incluyamos la exactitud o `\"acc\"` dentro de una lista 😄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdecf4b-310a-4914-a33d-5c9a11742102",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_iris.compile(optimizer=\"sgd\", loss=\"categorical_crossentropy\", metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d198b18-bcb1-4f0d-9fd0-baea01cdb7d8",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "También contamos con el método `fit` al igual que sklearn, pero debemos indicarle el número máximo de iteraciones o `epochs` y opcionalmente el `batch_size`, que indica el **tamaño de la muestra aleatoria** que usará para actualizar el gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30c38f3-7d55-4c6d-851d-9fdb9bfabe29",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_iris.fit(X_train, y_train, epochs=100, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c44a43-8db4-431b-8f89-f4258e8c5189",
   "metadata": {},
   "source": [
    "Para cada *epoch* tenemos 6 *batches*, $6 \\times 20 = 120$, el número de datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11514d69-1c8b-47f6-a757-c8be2a553715",
   "metadata": {},
   "source": [
    "## Evaluación\n",
    "El modelo tiene un método `evaluate` que puede evaluar según las métricas que definimos al configurar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c48b069-c6c9-492b-b352-ac0bde4f4937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicción y evaluación\n",
    "y_hat = model_iris.predict(X_test)\n",
    "model_iris.evaluate(X_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f746fa0f-bc56-4530-9f5f-8a247745c377",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "Cuando hablamos de *Deep Learning* o Aprendizaje profundo nos referimos al uso de redes neuronales para resolver tareas más complejas, estamos ante una **especialización** del *Machine Learning*\n",
    "\n",
    "![Deep learning dentro de ML](img/10.5_IA_ML_DL.png)\n",
    "\n",
    "Tareas como dotar a las máquinas la capacidad de **reconocer dígitos escritos a mano** ✍️🤯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ceede5-abe5-4eb8-bdc1-d969c1fc6096",
   "metadata": {},
   "source": [
    "# Reconociendo dígitos\n",
    "MNIST es un dataset con una gran cantidad de dígitos escritos a mano, son 70 mil 😱 en forma de imágenes de 28 x 28 pixeles.  \n",
    "Es toda una leyenda, por lo que muchas veces se utiliza como hola mundo 😄 y podemos cargarlo gracias a keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69059efd-fdaa-467f-a6be-604cf91846bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457e44a4-67ff-4cba-9407-e75c044fa449",
   "metadata": {},
   "source": [
    "La versión de keras ya cuenta con los conjuntos de entrenamiento y prueba, devueltos por la función `load_data` con una estructura un poco extraña para desempaquetar 🤔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceab4e39-08f7-4b50-958a-7526201b09dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = mnist.load_data()\n",
    "\n",
    "X_train, y_train = train\n",
    "X_test, y_test = test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c0f5d9-850a-4cda-81e1-1ac560dbb08a",
   "metadata": {},
   "source": [
    "Veamos el primer ejemplo de entrenamiento con `imshow` de matplotlib, el argumento `cmap=\"Greys\"` nos permite ver la imagen en escala de grises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57868dd-ad4a-450a-ae51-0c8fb866a220",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c930f-0bd5-4398-9d9b-85cf0395f980",
   "metadata": {},
   "source": [
    "Este es el número..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbefbf1-5436-4de9-a567-9b88b3bbbfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3709b00a-05c8-4c4f-bba0-6b93c87b2924",
   "metadata": {},
   "source": [
    "Además, ya tenemos los datos como deberíamos, en números 🔢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4c92c3-0c54-44ef-b2f7-11e838ae4eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c742eca8-b7a3-4b69-aa41-52977aab425f",
   "metadata": {},
   "source": [
    "## Preprocesamiento\n",
    "Tenemos **valores de 0 a 255**, los valores más altos son grises más oscuros. Pero **esta diferencia de valores puede dar problemas** con una red neuronal, por lo que un preprocesamiento común es **dividirlos por 255** para dejarlos en un **rango de 0 a 1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a2139a-6b8b-4580-a733-8428b72916c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_train / 255\n",
    "X_test_scaled = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efd290e-a304-4211-9070-bcb76f656768",
   "metadata": {},
   "source": [
    "Los labels están en un sólo número entero, pero deberían estar en One-hot encoding.  \n",
    "Por suerte to_categorical() de keras puede ayudarnos con eso 😉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a2f29c-be1a-4996-ad4f-fc11ab7f045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_train_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ddeb4b-4389-4c67-bf0e-1989691c3056",
   "metadata": {},
   "source": [
    "## Arquitectura de la red\n",
    "Es bastante sencillo definir la capa de salida, tenemos dígitos del 0 a 9 y nuestra tarea es clasificación multiclase, tendremos una **capa de salida con 10 neuronas y la activación softmax**.\n",
    "\n",
    "La entrada es una **matriz de 28 x 28**, esto son 784 números, usaremos ese número de neuronas en la capa de entrada para esta red, pero antes debemos \"aplanar\" esta matriz para convertirla en un sólo vector de 784 números.\n",
    "\n",
    "![aplanar matriz](https://i.imgur.com/dDYphPB.png)\n",
    "\n",
    "Podemos lograr esto usando una capa `Flatten` de keras, que recibirá un input shape de (28, 28).  \n",
    "Y para las **capas ocultas** puedes usar 1024 o 512 unidades 😃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbe755f-a503-42ba-a369-1ded11388f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "model_mnist = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(___, activation='relu'),\n",
    "    Dense(___, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afc69d7-522c-4880-918b-854e393b09fa",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "Usaremos las mismas opciones que en el caso de las Iris, cambiando el **batch size** a 128, también **se recomienda usar potencias de 2** para este argumento. En el caso anterior elegí 20 para dividir exactamente los 120 ejemplos de entrenamiento 😎."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f643d72c-78fa-4cb1-93a4-6d3cd5718031",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mnist.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics=['acc'])\n",
    "\n",
    "model_mnist.fit(X_train_scaled, y_train_cat, epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c60e58-2c26-461f-ac52-63aedad13ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model_mnist.predict(X_test_scaled)\n",
    "model_mnist.evaluate(X_test_scaled, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6471f5ee-5a1f-4ad6-9cec-9fc908a884ff",
   "metadata": {},
   "source": [
    "# Ejercicios\n",
    "Muestra un dígito y del conjunto de prueba junto a la predicción de la red neuronal 😉."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae40d9b7-1032-4c58-860d-83b1644a701a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfb00b94-8241-40ad-b466-409fd5139dad",
   "metadata": {},
   "source": [
    "El número de capas ocultas, el número de neuronas por capa y el batch size son **hiperparámetros** trata de ajustarlos para mejorar el rendimiento de la red que reconoce dígitos del mnist 😎."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44470c09-2cf7-458d-9951-f32429a04789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51640b90-fa49-495f-ae39-d4128bde1a14",
   "metadata": {},
   "source": [
    "# Epílogo\n",
    "El *Deep Learning* es una de las áreas que más crecimiento ha tenido en los últimos años, posibilita que las máquinas sean **capaces de aprender tareas mucho más complejas, sobre imágenes, texto, audio e incluso video** 🤯\n",
    "\n",
    "Tales cuestiones requieren de un amplio estudio, y de seguro estás con muchas ganas de aprender 😃.\n",
    "\n",
    "Esta colección de notebooks nació del deseo de compartir las bases del *Machine Learning*, sobre las que puedes apoyarte siempre que lo requieras, mi mayor deseo es haberte transmitido el entusiasmo que tengo por esta área y compartir un poco el cómo funciona 🤔.\n",
    "\n",
    "¡Gracias por quedarte hasta el final! 😊  \n",
    "Te deseo un feliz viaje por el mundo del Machine Learning 😃."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
