{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-possibility",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "! mkdir -p datasets\n",
    "%cd datasets\n",
    "! wget -nc https://raw.githubusercontent.com/pablonoya/zigzag-ml/master/datasets/housing.csv\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-poverty",
   "metadata": {},
   "source": [
    "# ¿Por qué crear features?\n",
    "Como mencionamos, la **elección de features** influye en el rendimento del modelo, y podemos encontrarnos en situaciones como tener medidas separadas de largo y ancho que podemos multiplicar para tener una sóla feature: el área, la cual será más fácil de manejar y representará a dos features a la vez.\n",
    "\n",
    "Y esta idea de usar la **multiplicación de features** puede ayudarnos en una situación aún más común, mira el siguiente gráfico que relaciona dos columnas de nuestro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_housing = pd.read_csv('./datasets/housing.csv')\n",
    "data_housing.dropna(inplace=True)\n",
    "data_housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-aircraft",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = data_housing['total_rooms']\n",
    "y = data_housing['housing_median_age']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel('total rooms')\n",
    "plt.ylabel('housing median age')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-spirituality",
   "metadata": {},
   "source": [
    "¿Puede una **recta ajustar estos datos** ?  \n",
    "Toma en cuenta que existen varios valores en el eje x, como si fueran otra recta.\n",
    "\n",
    "Si tuviéramos un trazo que baja de izquierda a derecha acercándose a la esquina inferior izquierda... ¡tendríamos una curva!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-vegetable",
   "metadata": {},
   "source": [
    "# Regresión polinomial\n",
    "Podemos describir una curva con la siguiente ecuación, que a su vez describe a un polinomio:\n",
    "\n",
    "$P(x) = a_nx^n + a_{n-1}x^{n-1} + \\cdots + a_0x^0$\n",
    "\n",
    "Notarás que se **parece a la regresión lineal múltiple**, con una pequeña gran diferencia, se usa la **misma feature elevada a un rango de exponentes**, que va de cero a un número **n**.\n",
    "\n",
    "Mientras más alto sea este número, denominado grado, más sinuosa será la gráfica.  \n",
    "![curves](https://www.themathpage.com/aPreCalc/Pre_Img/B12.png)\n",
    "\n",
    "Este procesamiento se conoce como **regresión polinomial**, probemos elevando a dos, incluyendo la misma feature al cuadrado.  \n",
    "Podemos \"apilar\" varios arrays contenidos en una tupla utilizando `column_stack` de numpy, esto tratará cada array como una columna, pero es necesario que todos los arrays tengan la misma longitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X_2D = X.values.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_2D, y, test_size=0.2)\n",
    "\n",
    "X_train_poly = np.column_stack((X_train ** 2, X_train))\n",
    "X_train_poly[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-forest",
   "metadata": {},
   "source": [
    "Ahora entrenemos un modelo, como si fuera una regresión lineal múltiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model_poly = LinearRegression()\n",
    "model_poly.fit(X_train_poly, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-geneva",
   "metadata": {},
   "source": [
    "## Veamos la curva\n",
    "Si unimos sólo 2 puntos tendremos una recta aunque usemos la ecuación polinómica de `model_poly`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpoints = np.array([min(X_train), max(X_train)])\n",
    "xpoints_poly = np.column_stack((xpoints ** 2, xpoints))\n",
    "\n",
    "ypoints = model_poly.predict(xpoints_poly)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.plot(xpoints, ypoints, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-wrist",
   "metadata": {},
   "source": [
    "Necesitamos valores intermedios, mientras más tengamos, más suave será nuestra curva. La función `linspace` de numpy nos permite generar $n$ **valores equidistantes** entre sí dentro de un rango definido.\n",
    "\n",
    "Generemos 5 valores, el rango será el mínimo y el máximo de X_train, que contiene los valores originales, sin elevar al cuadrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpoints = np.linspace(start=min(X_train), stop=max(X_train), num=5)\n",
    "xpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-judgment",
   "metadata": {},
   "source": [
    "Utilizemos estos puntos intermedios para graficar la curva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpoints_poly = np.column_stack((xpoints ** 2, xpoints))\n",
    "ypoints = model_poly.predict(xpoints_poly)\n",
    "\n",
    "plt.plot(xpoints, ypoints, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-stability",
   "metadata": {},
   "source": [
    "Si deseamos suavizar la curva no es necesario generar **demasiados** valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpoints = np.linspace(start=min(X_train), stop=max(X_train), num=20)\n",
    "xpoints_poly = np.column_stack((xpoints ** 2, xpoints))\n",
    "ypoints = model_poly.predict(xpoints_poly)\n",
    "\n",
    "plt.plot(xpoints, ypoints, color=\"red\")\n",
    "plt.scatter(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-newark",
   "metadata": {},
   "source": [
    "## Genera polinomios\n",
    "El objeto [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html?highlight=polynomial#sklearn-preprocessing-polynomialfeatures) de sklearn convierte las features en polinomios, el parámetro `degree` indica el [grado](https://es.wikipedia.org/wiki/Grado_(polinomio)), esto es, el exponente máximo al que elevaremos.\n",
    "\n",
    "Su método `fit_transform` nos devolverá una matriz con cada fila en el formato $x^0, x^1, \\cdots, x^n$  si el parámetro es **una** sola feature.  \n",
    "De manera interna, este método llama primero a `fit` para calcular qué features tendremos y luego a `transform` para crear dichas features, esta separación es importante en otro tipo de procedimientos, pues sólo deberíamos usar el conjunto de entrenamiento para realizar ajustes y el de prueba para las pruebas.\n",
    "\n",
    "Como $x^0 = 1$ tendremos una columna llena de unos, este término se conoce como **bias** o sesgo, ¡un termino que será muy importante!, pero que ahora excluiremos estableciendo `include_bias` en falso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-pixel",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_train_poly[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-indication",
   "metadata": {},
   "source": [
    "Si decidimos incluir varias features tendremos un **polinomio de varias variales**.  \n",
    "Veamos uno de **dos variables y grado dos**, sin coeficientes:\n",
    "\n",
    "${x_1}^2 + x_1 + x_1x_2 + x_2 +{x_2}^2$\n",
    "\n",
    "El exponente máximo es dos, aunque el término del medio tenga las dos variables, la suma de sus exponentes es 2 debido a que ambos tienen grado 1.  \n",
    "Así, un término con la forma ${x_1}^2x_2$ tendría grado 3, por la suma de sus exponentes.\n",
    "\n",
    "En nuestro caso de dos variables y grado 2 `PolynomialFeatures` devolverá una lista con el formato $x_1, x_2, {x_1}^2, x_1x_2, {x_2}^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambiamos de grado\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# tomamos dos features\n",
    "X_selected = data_housing[['total_rooms', 'total_bedrooms']]\n",
    "\n",
    "# recuerda que sólo tomamos una parte del dataset\n",
    "X_train2 = X_selected[:len(X_train)]\n",
    "\n",
    "X_train_poly = poly.fit_transform(X_train2)\n",
    "X_train_poly[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ad572a-8947-451a-a3d6-acf7b202457e",
   "metadata": {},
   "source": [
    "## ¡Pero no generes tantos!\n",
    "Prueba aumentar el grado, verás más columnas, y muchas más si añades features antes de generar polinomios 😱.\n",
    "\n",
    "Estas columnas cuentan como nuevas features para el modelo, nuevas features pueden disminuir el error, pero por cada columna **añades una dimensión** y esto causa un problema conocido como *curse of dimensionality* o maldición de la dimensión.\n",
    "\n",
    "Esto trae \"problemas que no se ven en menores dimensiones\", como la **dispesión** y la **cercanía** de los datos.  \n",
    "\n",
    "Lo explicaré mejor con un pequeño juego imaginario.\n",
    "Ahoras estás en el principio de un espacio de 4 secciones y debes debes alcanzar un objeto que está al final:\n",
    "\n",
    "    O--*\n",
    "    \n",
    "Sólo debes recorrer 3 secciones para alcanzarlo, estás muy cerca del objeto, lo alcanzas y pasas de nivel.\n",
    "\n",
    "Ahora añadimos otra dimensión, tienes un espacio de 3 x 3, con 9 secciones en total:\n",
    "\n",
    "    XX*\n",
    "    XXX\n",
    "    OXX\n",
    "\n",
    "Ahora debes recorrer 4 secciones para alcanzar el objeto, ya no está tan cerca pero no tardas en alcanzarlo y pasar a otro nivel.\n",
    "\n",
    "Se añade otra dimensión también con 3 secciones, tendemos un juego 3D con 27 secciones y la misma situación.\n",
    "¿Cuántas secciones debes recorrer? \n",
    "\n",
    "Al añadir dimensiones, tú y el objeto se han **dispersado**, perdiendo **cercanía**, lo mismo pasa con los datos cuando añadimos features, será más dificil para el modelo encontrar la recta que mejor se ajuste porque tenemos distancias más grandes entre los puntos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33f5075-cde4-4d5d-bc27-97e35695201b",
   "metadata": {},
   "source": [
    "## A menos que...\n",
    "Si contamos con **más datos** de los que aprender, un modelo **más complejo** rendirá mejor, una mayor cantidad de datos nos ayudará a ver un panorama más general. Este aspecto lo trataremos mejor más adelante 😉."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-storage",
   "metadata": {},
   "source": [
    "# Probando la curva\n",
    "Veamos si el modelo polinomial se desempeña mejor al final, para esto es necesario transformar también el test set, usaremos sólo `transform` pues el ajuste que corresponde a la función `fit` se debe realizar en el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "model_poly.fit(X_train_poly, y_train)\n",
    "\n",
    "y_hat_poly_train = model_poly.predict(X_train_poly)\n",
    "y_hat_poly = model_poly.predict(X_test_poly)\n",
    "\n",
    "print(\"MSE train poly\", mean_squared_error(y_hat_poly_train, y_train))\n",
    "print(\"MSE test poly\", mean_squared_error(y_hat_poly, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450ad366-6afa-4f43-9d24-fe8ad0f57c73",
   "metadata": {},
   "source": [
    "Para comparar, entrenamos un modelo de regresión lineal simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_hat_train = model.predict(X_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "print(\"MSE train\", mean_squared_error(y_hat_train, y_train))\n",
    "print(\"MSE test\", mean_squared_error(y_hat, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf78b03-9c21-4b0c-b2ee-2a118c6be15e",
   "metadata": {},
   "source": [
    "# Ejercicios\n",
    "Prueba generar polinomios con grados más altos y ejecuta las pruebas, ¿las métricas mejoran?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2c76a-c142-4f9e-a513-531667b6139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sólo cambia un parámetro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec025675-0104-40da-bf31-8bb1f9cca6e2",
   "metadata": {},
   "source": [
    "Desde el principio no incluímos nuestra columna objetivo, la del precio, inclúyela junto a una o más features para generar un poliniomio que mejore el modelo del capítulo anterior 😎."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dbfaeb-98fa-49bc-8c0b-65ab71bc74d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambia y, divide en conjuntos y entrena con los polinomios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-printer",
   "metadata": {},
   "source": [
    "Es cierto que un modelo más complejo permite aprender mejor, pero si aprendes mejor sólo unas cuantas cosas... estarías perdiendo la idea general.  \n",
    "Lo mismo pasa con machine learning, es importante que lo aprendido se pueda **generalizar**, que el conocimiento funcione no sólo con lo que hemos visto, de lo contrario tenemos un problema llamado **overfitting** o [sobreajuste](./5_overfitting.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
