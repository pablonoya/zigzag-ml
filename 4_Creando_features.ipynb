{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-possibility",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "! mkdir -p datasets\n",
    "%cd datasets\n",
    "! wget -nc https://raw.githubusercontent.com/pablonoya/zigzag-ml/master/datasets/housing.csv\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-poverty",
   "metadata": {},
   "source": [
    "# 쯇or qu칠 crear features?\n",
    "Como mencionamos, la **elecci칩n de features** influye en el rendimento del modelo, y podemos encontrarnos en situaciones como tener medidas separadas de largo y ancho que podemos multiplicar para tener una s칩la feature: el 치rea, la cual ser치 m치s f치cil de manejar y representar치 a dos features a la vez.\n",
    "\n",
    "Y esta idea de usar la **multiplicaci칩n de features** puede ayudarnos en una situaci칩n a칰n m치s com칰n, mira el siguiente gr치fico que relaciona dos columnas de nuestro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_housing = pd.read_csv('./datasets/housing.csv')\n",
    "data_housing.dropna(inplace=True)\n",
    "data_housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-aircraft",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = data_housing['total_rooms']\n",
    "y = data_housing['housing_median_age']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel('total rooms')\n",
    "plt.ylabel('housing median age')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-spirituality",
   "metadata": {},
   "source": [
    "쯇uede una **recta ajustar estos datos** ?  \n",
    "Toma en cuenta que existen varios valores en el eje x, como si fueran otra recta.\n",
    "\n",
    "Si tuvi칠ramos un trazo que baja de izquierda a derecha acerc치ndose a la esquina inferior izquierda... 춰tendr칤amos una curva!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-vegetable",
   "metadata": {},
   "source": [
    "# Regresi칩n polinomial\n",
    "Podemos describir una curva con la siguiente ecuaci칩n, que a su vez describe a un polinomio:\n",
    "\n",
    "$P(x) = a_nx^n + a_{n-1}x^{n-1} + \\cdots + a_0x^0$\n",
    "\n",
    "Notar치s que se **parece a la regresi칩n lineal m칰ltiple**, con una peque침a gran diferencia, se usa la **misma feature elevada a un rango de exponentes**, que va de cero a un n칰mero **n**.\n",
    "\n",
    "Mientras m치s alto sea este n칰mero, denominado grado, m치s sinuosa ser치 la gr치fica.  \n",
    "![curves](https://www.themathpage.com/aPreCalc/Pre_Img/B12.png)\n",
    "\n",
    "Este procesamiento se conoce como **regresi칩n polinomial**, probemos elevando a dos, incluyendo la misma feature al cuadrado.  \n",
    "Podemos \"apilar\" varios arrays contenidos en una tupla utilizando `column_stack` de numpy, esto tratar치 cada array como una columna, pero es necesario que todos los arrays tengan la misma longitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X_2D = X.values.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_2D, y, test_size=0.2)\n",
    "\n",
    "X_train_poly = np.column_stack((X_train ** 2, X_train))\n",
    "X_train_poly[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-forest",
   "metadata": {},
   "source": [
    "Ahora entrenemos un modelo, como si fuera una regresi칩n lineal m칰ltiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model_poly = LinearRegression()\n",
    "model_poly.fit(X_train_poly, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-geneva",
   "metadata": {},
   "source": [
    "## Veamos la curva\n",
    "Si unimos s칩lo 2 puntos tendremos una recta aunque usemos la ecuaci칩n polin칩mica de `model_poly`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpoints = np.array([min(X_train), max(X_train)])\n",
    "xpoints_poly = np.column_stack((xpoints ** 2, xpoints))\n",
    "\n",
    "ypoints = model_poly.predict(xpoints_poly)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.plot(xpoints, ypoints, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-wrist",
   "metadata": {},
   "source": [
    "Necesitamos valores intermedios, mientras m치s tengamos, m치s suave ser치 nuestra curva. La funci칩n `linspace` de numpy nos permite generar $n$ **valores equidistantes** entre s칤 dentro de un rango definido.\n",
    "\n",
    "Generemos 5 valores, el rango ser치 el m칤nimo y el m치ximo de X_train, que contiene los valores originales, sin elevar al cuadrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpoints = np.linspace(start=min(X_train), stop=max(X_train), num=5)\n",
    "xpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-judgment",
   "metadata": {},
   "source": [
    "Utilizemos estos puntos intermedios para graficar la curva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpoints_poly = np.column_stack((xpoints ** 2, xpoints))\n",
    "ypoints = model_poly.predict(xpoints_poly)\n",
    "\n",
    "plt.plot(xpoints, ypoints, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-stability",
   "metadata": {},
   "source": [
    "Si deseamos suavizar la curva no es necesario generar **demasiados** valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpoints = np.linspace(start=min(X_train), stop=max(X_train), num=20)\n",
    "xpoints_poly = np.column_stack((xpoints ** 2, xpoints))\n",
    "ypoints = model_poly.predict(xpoints_poly)\n",
    "\n",
    "plt.plot(xpoints, ypoints, color=\"red\")\n",
    "plt.scatter(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-newark",
   "metadata": {},
   "source": [
    "## Genera polinomios\n",
    "El objeto [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html?highlight=polynomial#sklearn-preprocessing-polynomialfeatures) de sklearn convierte las features en polinomios, el par치metro `degree` indica el [grado](https://es.wikipedia.org/wiki/Grado_(polinomio)), esto es, el exponente m치ximo al que elevaremos.\n",
    "\n",
    "Su m칠todo `fit_transform` nos devolver치 una matriz con cada fila en el formato $x^0, x^1, \\cdots, x^n$  si el par치metro es **una** sola feature.  \n",
    "De manera interna, este m칠todo llama primero a `fit` para calcular qu칠 features tendremos y luego a `transform` para crear dichas features, esta separaci칩n es importante en otro tipo de procedimientos, pues s칩lo deber칤amos usar el conjunto de entrenamiento para realizar ajustes y el de prueba para las pruebas.\n",
    "\n",
    "Como $x^0 = 1$ tendremos una columna llena de unos, este t칠rmino se conoce como **bias** o sesgo, 춰un termino que ser치 muy importante!, pero que ahora excluiremos estableciendo `include_bias` en falso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-pixel",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_train_poly[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-indication",
   "metadata": {},
   "source": [
    "Si decidimos incluir varias features tendremos un **polinomio de varias variales**.  \n",
    "Veamos uno de **dos variables y grado dos**, sin coeficientes:\n",
    "\n",
    "${x_1}^2 + x_1 + x_1x_2 + x_2 +{x_2}^2$\n",
    "\n",
    "El exponente m치ximo es dos, aunque el t칠rmino del medio tenga las dos variables, la suma de sus exponentes es 2 debido a que ambos tienen grado 1.  \n",
    "As칤, un t칠rmino con la forma ${x_1}^2x_2$ tendr칤a grado 3, por la suma de sus exponentes.\n",
    "\n",
    "En nuestro caso de dos variables y grado 2 `PolynomialFeatures` devolver치 una lista con el formato $x_1, x_2, {x_1}^2, x_1x_2, {x_2}^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambiamos de grado\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# tomamos dos features\n",
    "X_selected = data_housing[['total_rooms', 'total_bedrooms']]\n",
    "\n",
    "# recuerda que s칩lo tomamos una parte del dataset\n",
    "X_train2 = X_selected[:len(X_train)]\n",
    "\n",
    "X_train_poly = poly.fit_transform(X_train2)\n",
    "X_train_poly[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ad572a-8947-451a-a3d6-acf7b202457e",
   "metadata": {},
   "source": [
    "## 춰Pero no generes tantos!\n",
    "Prueba aumentar el grado, ver치s m치s columnas, y muchas m치s si a침ades features antes de generar polinomios 游땸.\n",
    "\n",
    "Estas columnas cuentan como nuevas features para el modelo, nuevas features pueden disminuir el error, pero por cada columna **a침ades una dimensi칩n** y esto causa un problema conocido como *curse of dimensionality* o maldici칩n de la dimensi칩n.\n",
    "\n",
    "Esto trae \"problemas que no se ven en menores dimensiones\", como la **dispesi칩n** y la **cercan칤a** de los datos.  \n",
    "\n",
    "Lo explicar칠 mejor con un peque침o juego imaginario.\n",
    "Ahoras est치s en el principio de un espacio de 4 secciones y debes debes alcanzar un objeto que est치 al final:\n",
    "\n",
    "    O--*\n",
    "    \n",
    "S칩lo debes recorrer 3 secciones para alcanzarlo, est치s muy cerca del objeto, lo alcanzas y pasas de nivel.\n",
    "\n",
    "Ahora a침adimos otra dimensi칩n, tienes un espacio de 3 x 3, con 9 secciones en total:\n",
    "\n",
    "    XX*\n",
    "    XXX\n",
    "    OXX\n",
    "\n",
    "Ahora debes recorrer 4 secciones para alcanzar el objeto, ya no est치 tan cerca pero no tardas en alcanzarlo y pasar a otro nivel.\n",
    "\n",
    "Se a침ade otra dimensi칩n tambi칠n con 3 secciones, tendemos un juego 3D con 27 secciones y la misma situaci칩n.\n",
    "쮺u치ntas secciones debes recorrer? \n",
    "\n",
    "Al a침adir dimensiones, t칰 y el objeto se han **dispersado**, perdiendo **cercan칤a**, lo mismo pasa con los datos cuando a침adimos features, ser치 m치s dificil para el modelo encontrar la recta que mejor se ajuste porque tenemos distancias m치s grandes entre los puntos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33f5075-cde4-4d5d-bc27-97e35695201b",
   "metadata": {},
   "source": [
    "## A menos que...\n",
    "Si contamos con **m치s datos** de los que aprender, un modelo **m치s complejo** rendir치 mejor, una mayor cantidad de datos nos ayudar치 a ver un panorama m치s general. Este aspecto lo trataremos mejor m치s adelante 游땔."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-storage",
   "metadata": {},
   "source": [
    "# Probando la curva\n",
    "Veamos si el modelo polinomial se desempe침a mejor al final, para esto es necesario transformar tambi칠n el test set, usaremos s칩lo `transform` pues el ajuste que corresponde a la funci칩n `fit` se debe realizar en el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "model_poly.fit(X_train_poly, y_train)\n",
    "\n",
    "y_hat_poly_train = model_poly.predict(X_train_poly)\n",
    "y_hat_poly = model_poly.predict(X_test_poly)\n",
    "\n",
    "print(\"MSE train poly\", mean_squared_error(y_hat_poly_train, y_train))\n",
    "print(\"MSE test poly\", mean_squared_error(y_hat_poly, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450ad366-6afa-4f43-9d24-fe8ad0f57c73",
   "metadata": {},
   "source": [
    "Para comparar, entrenamos un modelo de regresi칩n lineal simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_hat_train = model.predict(X_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "print(\"MSE train\", mean_squared_error(y_hat_train, y_train))\n",
    "print(\"MSE test\", mean_squared_error(y_hat, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf78b03-9c21-4b0c-b2ee-2a118c6be15e",
   "metadata": {},
   "source": [
    "# Ejercicios\n",
    "Prueba generar polinomios con grados m치s altos y ejecuta las pruebas, 쯟as m칠tricas mejoran?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2c76a-c142-4f9e-a513-531667b6139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s칩lo cambia un par치metro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec025675-0104-40da-bf31-8bb1f9cca6e2",
   "metadata": {},
   "source": [
    "Desde el principio no inclu칤mos nuestra columna objetivo, la del precio, incl칰yela junto a una o m치s features para generar un poliniomio que mejore el modelo del cap칤tulo anterior 游땙."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dbfaeb-98fa-49bc-8c0b-65ab71bc74d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambia y, divide en conjuntos y entrena con los polinomios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-printer",
   "metadata": {},
   "source": [
    "Es cierto que un modelo m치s complejo permite aprender mejor, pero si aprendes mejor s칩lo unas cuantas cosas... estar칤as perdiendo la idea general.  \n",
    "Lo mismo pasa con machine learning, es importante que lo aprendido se pueda **generalizar**, que el conocimiento funcione no s칩lo con lo que hemos visto, de lo contrario tenemos un problema llamado **overfitting** o [sobreajuste](./5_overfitting.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
