{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "! mkdir -p datasets\n",
    "%cd datasets\n",
    "! wget -nc https://raw.githubusercontent.com/pablonoya/zigzag-ml/master/datasets/housing.csv\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¬°Hola mundo! del machine learning\n",
    "Considero que el [hola mundo](https://es.wikipedia.org/wiki/Hola_mundo) del machine learning es la [regresi√≥n lineal](https://es.wikipedia.org/wiki/Regresi%C3%B3n_lineal) un modelo matem√°tico que busca aproximar la relaci√≥n de una variable que depende de otras. Es una forma de saber c√≥mo **las entradas influyen en la salida** ü§î.\n",
    "\n",
    "El modelo resultante ser√° la recta que mejor se ajuste a nuestros datos üìâ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## y = mx + b\n",
    "Es la ecuaci√≥n de la recta, describe la relacion entre un **t√©rmino independiente** $x$ y uno **dependiente** $y$ que depende de $x$, el **par√°metro** $m$ y el t√©rmino independiente \"$b$.  \n",
    "Donde:\n",
    "\n",
    "- $m$ es la **pendiente**, mide qu√© tan inclinada est√° la recta, un valor negativo muestra una recta que baja de izquierda a derecha.\n",
    "- $b$ es el **t√©rmino independiente**, es el punto en el que la recta corta el eje vertical de un plano.\n",
    "- $y$ es la variable **dependiente** de los dem√°s valores.\n",
    "- $x$ es la variable **independiente**.\n",
    "\n",
    "Si la ecuaci√≥n fuera $y = -\\frac{1}{2}x + 2$\n",
    "La recta resultante ser√≠a:  \n",
    "![recta resultante](img/2.1_line_equation.png)\n",
    "\n",
    "Por tanto, $m = \\frac{1}{2}$ y  $b = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolviendo el problema\n",
    "Podemos resolver el primer problema ~~salvaje~~ que nos apareci√≥, predecir el precio de una casa. Utilizaremos el dataset [California Housing](https://www.kaggle.com/camnugent/california-housing-prices), este dataset es el primer ejemplo del libro [Hands-on Machine Learning with Scikit-Learn and Tensorflow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) ü¶éüìñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¬°venga, dataset!\n",
    "import pandas as pd\n",
    "data_housing = pd.read_csv('./datasets/housing.csv')\n",
    "data_housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mirando el dataset\n",
    "Antes de utilizar los datos deber√≠amos verificarlos un poco, por lo menos verificando que no existan datos vac√≠os.\n",
    "Recordar√°s que podemos usar el m√©todo `isna`, pero nuestra tabla ahora es muy grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_housing.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos 20 mil filas üò® revisarlas una a una para ver si tienen valores vac√≠os no es una opci√≥n, lo mejor ser√≠a contar qu√© valores son verdaderos, una tarea f√°cil si utilizamos una sumatoria pues Python convierte los valores True a 1 y False a 0, y podemos sumar por columna utilizando el m√©todo `sum` del DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_housing.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ahora vamos a borrar cada fila que tenga un valor vac√≠o usando `dropna`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_housing.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contamos con 10 columnas, cada fila describe las caracter√≠sticas promedio de un bloque de casas üè°üè°üè°.  \n",
    "Tomemos una columna como feature, la que mejor parece determinar el precio, dado por \"median_house_value\" es el ingreso en miles de sus habitantes, dado por \"median_income\". Cada columna retorna un objeto Series, y vamos a asignarlas en 2 variables.\n",
    "> Features en `X`, y labels en `y`, as√≠ nombraremos las variables desde ahora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionando columnas tendremos objetos Series\n",
    "X = data_housing['median_income']\n",
    "y = data_housing['median_house_value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Veamos c√≥mo se relacionan ambas columnas gracias a una [gr√°fica de dispersion](https://es.wikipedia.org/wiki/Diagrama_de_dispersi%C3%B3n) realizada con [matplotlib](https://matplotlib.org/), una librer√≠a para graficar datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel(\"Ingreso\")\n",
    "plt.ylabel(\"Precio\")\n",
    "plt.title(\"Relaci√≥n entre ingreso y precio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, vemos una **relaci√≥n directamente proporcional**: a mayor ingreso, mayor costo.  \n",
    "Tambi√©n vemos muchos puntos en la parte superior, esto parece indicar que los precios mayores a 500 mil han sido truncados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **relaci√≥n** entre el **par√°metro y t√©rmino independiente** de la recta y los atributos del objeto `model` es:\n",
    "- $m$ es igual a `model.coef_[0]`\n",
    "- $b$ es igual a `model.intercept_`\n",
    "\n",
    "Debido a esto, cuando **entrenamos un modelo** de machine learning, en realidad, buscamos **hallar los par√°metros que mejor se ajusten** a los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# reshape a 2D considerando que trabajamos con un Series\n",
    "X_2D = X.values.reshape(-1, 1)\n",
    "\n",
    "# entrenamos el modelo para hallar los par√°metros de la recta\n",
    "model = LinearRegression()\n",
    "model.fit(X_2D, y)\n",
    "\n",
    "print(f\"Los par√°metros obtenidos son: m = {model.coef_[0] :.2f} y b = {model.intercept_ :.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **pendiente es positiva**, esto confirma la relaci√≥n **directamente proporcional** ‚Üó."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizando el modelo\n",
    "Con el modelo entrenado, podemos **estimar** o **predecir** el valor de todas las viviendas, s√≥lo debemos llamar al m√©todo `predict`. Este usar√° internamente los par√°metros `coef_` e `intercept` para la ecuaci√≥n que ya conocemos üòé.\n",
    "\n",
    "Para evaluar al modelo comparemos la primera predicci√≥n con el valor real de la primera vivienda. Los valores reales son los targets, recuerda que est√°n en `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values = model.predict(X_2D)\n",
    "\n",
    "# comparemos con el valor real\n",
    "print(f\"El valor real es {y[0]}, y seg√∫n el modelo es {predicted_values[0] :.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay una diferencia de -59 573 para esta vivienda, el modelo estima que el valor debe ser m√°s bajo porque la recta est√° debajo del valor real y la diferencia ser√° mayor mientras m√°s lejos est√© el valor real de la recta.\n",
    "\n",
    "Vamos a **graficar la recta** que mejor se ajust√≥ a los datos, tomaremos los valores m√°ximo y m√≠nimo de `X_2D`, para tener los l√≠mites de la gr√°fica.  \n",
    "Entonces aplicamos la **predicci√≥n del modelo** para tener su respectivos valores en el eje y, entonces uniremos ambos puntos con un **gr√°fico de lineas** usando la funci√≥n `plot` de matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la gr√°fica de los datos de fondo\n",
    "plt.scatter(X, y)\n",
    "\n",
    "xpoints = [min(X_2D), max(X_2D)]\n",
    "ypoints = model.predict(xpoints)\n",
    "\n",
    "plt.plot(xpoints, ypoints, color=\"red\")\n",
    "plt.xlabel(\"Ingreso\")\n",
    "plt.ylabel(\"Precio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerando que los valores son altos, hasta 500 mil, no debemos preocuparnos mucho por un \"peque√±o\" error de -59 mil.  \n",
    "Pero esto s√≥lo aplica a la primera vivienda, **¬øc√≥mo evaluamos el error de todas?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluando el modelo\n",
    "Podr√≠amos promediar todos los errores, pero los que est√©n **debajo de la recta ser√°n negativos**, como en la primera casa, para solucionarlo, **elevamos cada error al cuadrado** y luego promediamos estos.\n",
    "\n",
    "$$\\frac{1}{n} \\sum{(y_{r} - \\widehat{y} )^2}$$\n",
    "\n",
    "\n",
    "$\\widehat{y}$ se lee **y hat** (en espa√±ol, *ye sombrero* ü§†) es el valor que predice el modelo, esto nos muestra que medimos la distancia vertical de cada punto a la recta para calcular el error. La recta que **mejor se ajusta** es aquella que **minimiza las distancias** hacia todos los puntos.\n",
    "\n",
    "Esta f√≥rmula es una **m√©trica** conocida como *mean squared error* (**MSE** para los amigos) o error cuadr√°tico promedio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_2D)\n",
    "\n",
    "squared_errors = (y_hat - y) ** 2\n",
    "mse = sum(squared_errors) / len(y)\n",
    "\n",
    "print(f\"MSE: {mse :.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬°Es bastante alto! üò∞\n",
    "\n",
    "Si un error es alto, este valor al cuadrado es mucho m√°s alto. Podemos observar **outliers** o valores extremos que se alejan bastante de la recta, si los sumamos... tendremos el porqu√© ese error tan alto.  \n",
    "Es frecuente calcular el **square root of MSE (RMSE)** o ra√≠z cuadrada del MSE para tener una mejor idea del error real, sin embargo este sigue siendo afectado por los outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mse ** (1/2)\n",
    "print(f\"RMSE: {rmse :.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much√≠simo mejor üòå\n",
    "\n",
    "Adem√°s, existe una m√©trica a la que no le afectan demasiado los outliers, *mean absolute error* (**MAE**) o error absoluto promedio, es otra soluci√≥n para los errores negativos, pues quiz√° hayas pensado tambi√©n en el valor absoluto para solucionar el problema de obtener el promedio con n√∫meros negativos.\n",
    "\n",
    "Tanto MSE como MAE se pueden calcular con `sklearn.metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "print(f\"MAE: {mean_absolute_error(y_hat, y) :.2f}\")\n",
    "print(f\"MSE: {mean_squared_error(y_hat, y) :.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incluyendo m√°s features\n",
    "¬øRecuerdas que sklearn espera un array 2D? cada feature que incluyamos es una columna m√°s, con dos o m√°s features:\n",
    "\n",
    "- Entrenamos una regresi√≥n lineal **m√∫ltiple**, depende de muchas variables.\n",
    "- El atributo `coef_` del modelo tendr√° **m√∫ltiples features** (siempre las tuvo, pero s√≥lo tomamos el primer valor).\n",
    "- Es posible que **mejore el modelo**, si eliges **features que aporten informaci√≥n relevante** a la predicci√≥n.\n",
    "\n",
    "Incluyamos la feature \"housing_median_age\" que describe la edad promedio de las casas, ambas features estar√°n en un DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todas las filas, sexta y octava columna\n",
    "X_selected_features = data_housing[['median_income', 'housing_median_age']]\n",
    "X_selected_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrenamos otro modelo\n",
    "model_selected_features = LinearRegression()\n",
    "\n",
    "# ¬°ya no es necesario el reshape!\n",
    "model_selected_features.fit(X_selected_features, y)\n",
    "\n",
    "print(\"Coeficientes: \", model_selected_features.coef_)\n",
    "\n",
    "# evaluemos\n",
    "y_hat = model_selected_features.predict(X_selected_features)\n",
    "print(f\"MAE: {mean_absolute_error(y_hat, y) :.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬°S√≠ mejor√≥! ü•≥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intenta **cambiar o incluir m√°s features**, pero no todas las combinaciones dar√°n un mejor modelo, por razones que trataremos m√°s adelante. Adem√°s no puedes usar (a√∫n) la √∫ltima columna \"ocean_proximity\" porque **no contiene valores n√∫mericos** .  \n",
    "Tampoco podr√°s graficar en 2D si escoges 2 o m√°s features, pero siempre podr√°s usar alguna m√©trica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresi√≥n lineal m√∫ltiple\n",
    "Como mencionamos, el modelo ahora tiene ese nombre y permite aprender de m√∫ltiples _features_.\n",
    "\n",
    "Un gran nombre conlleva una gran ecuaci√≥n: \n",
    "\n",
    "$$y = a_1x_1 + a_2x_2 + ... + a_nx_n + b $$\n",
    "\n",
    "Donde:\n",
    "+ $n$ es el **n√∫mero** de features.\n",
    "+ $a$ es un **vector de par√°metros**.\n",
    "+ $x$ es el **vector de features** para cada fila.\n",
    "+ $b$ es el **t√©rmino independiente**.\n",
    "\n",
    "La cual es una **generalizaci√≥n** de la ecuaci√≥n de la recta, conocida tambi√©n como ecuaci√≥n lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios\n",
    "Algunas combinaciones de features funcionar√°n mejor que otras, **prueba combinaciones de dos features**.  \n",
    "¬øCu√°l parece funcionar mejor sobre la m√©trica MSE? ¬øA qu√© crees que se deba?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables para las columnas de features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrena** un modelo con las mejores features que hallaste, y **utiliza** los atributos en `coef_` e `intercept` para **implementar** la ecuaci√≥n de regresi√≥n lineal m√∫ltiple.  \n",
    "Por √∫ltimo, **elige alguna fila** de los datos existentes para comparar el resultado con la respuesta real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crea un nuevo modelo para esta tarea\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¬øEs un buen modelo?\n",
    "Las m√©tricas son √∫tiles, pero pueden ser **enga√±osas**, debemos elegir siempre la que m√°s se adec√∫e al problema, no la que presente un **error m√°s bajo** (no hagas trampa üò°).  \n",
    "\n",
    "Lo importante es que te hagas preguntas como ¬ødeber√≠a tomar en cuenta los **outliers**? o ¬ønecesito que mi modelo sea muy **preciso**?.  \n",
    "La respuesta depender√° de los objetivos del modelo, y este **siempre depender√° de los datos** pues el modelo matem√°tico ser√° el mismo pero con **diferentes par√°metros** como resultado de su entrenamiento üß†.\n",
    "\n",
    "Las **m√©tricas suelen ser lo √∫ltimo** que extraemos de un proyecto de machine learning, antes deber√≠amos [probarlo](./3_test_set.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
