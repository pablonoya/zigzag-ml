{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "! mkdir -p datasets\n",
    "%cd datasets\n",
    "! wget -nc https://raw.githubusercontent.com/pablonoya/zigzag-ml/master/datasets/housing.csv\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting y Underfitting\n",
    "Son dos problemas que pueden ocurrir luego de haber entrenado al modelo.  \n",
    "El *overfitting*, o **sobreajuste**, ocurre cuando el modelo se ajusta demasiado a los datos de entrenamiento, como una curva que intenta cruzar por la mayor cantidad de puntos posible. Se lo conoce tambi√©n como *high variance* o **varianza alta** .  \n",
    "En cambio el *underfitting*, o ajuste d√©bil, sucede cuando el modelo no se ajusta bien a los datos, como una recta que no puede alcanzar gran parte de los datos. Se lo conoce tambi√©n como *high bias* o **sesgo elevado**.  \n",
    "\n",
    "![Ajuste debil, ideal y sobreajuste](img/5.0.1_Overfitting_right_underfitting.png)\n",
    "\n",
    "¬øRecuerdas que una recta no bastaba? un polinomio de grado 3 fue mejor, y un polinomio de alto grado se ajusta demasiado a los datos, por el hecho de tener m√°s curvas.\n",
    "\n",
    "**Incluir m√°s _features_ aumenta la complejidad** del modelo, m√°s par√°metros para entrenar dejan **mayor libertad** de valores para aprender, y si esta es alta podemos llegar al **sobreajuste**. Por el contrario, **reducir las features** causa que el modelo tenga **menos libertad** y aprenda de poca informaci√≥n, lo que deriva en **ajuste debil**.  \n",
    "\n",
    "Debido a esto es **importante mezclar los datos y tener la mayor variedad posible** en nuestros conjuntos de entrenamiento y prueba üòâ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-variance trade-off\n",
    "Esta situaci√≥n que depende de las *features* nos indica que hay una **compensaci√≥n** entre **varianza** y **_bias_**, incrementar la varianza disminuye el sesgo y viceversa üîÅ.  \n",
    "Un modelo con **alto bias** es m√°s simple y realizar√° predicciones simples, como la **primera** gr√°fica.\n",
    "Uno complejo tendr√° **alta varianza** realizando predicciones m√°s complejas que pueden ser muy variadas, como la **tercera** gr√°fica.\n",
    "\n",
    "Por lo que debemos encontrar un **balance** entre ambos, el modelo no debe ser demasiado simple ni demasiado complejo, como en la **segunda** gr√°fica. Esto asegura que no perdamos el panorama general üòÉ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¬øQu√© est√° mal en mi modelo?\n",
    "Para saber a qu√© problema nos enfrentamos es necesario hacer pruebas con ambos conjuntos, tendremos **_overfitting_** cuando el **error en uno de los conjuntos sea mucho menor**, con frecuencia es el de entrenamiento üò®.  \n",
    "Y **_underfitting_** cuando el **error de ambos conjuntos sea elevado** üò∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solucionando el underfitting\n",
    "La recomendaci√≥n general es **incrementar la complejidad del modelo**, esto se logra a√±adiendo m√°s features, o subiendo el grado polin√≥mico al que elevemos algunas.  \n",
    "Es importante tomar en cuenta que **las features que a√±adamos deben estar siempre disponibles**, caso contrario, no podremos utilizar el modelo en la vida real ‚òπ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solucionando el overfit\n",
    "Lo contrario ser√≠a **disminuir la complejidad**, pero aqu√≠ tenemos m√°s opciones, como **entrenar utilizando mayor cantidad de datos** lo cual ayuda a no perder el objetivo general, o usar **regularizaci√≥n**, el nombre se debe a que regularemos la libertad de los coeficientes para que **no sean tan altos**, reduciendo la varianza.\n",
    "\n",
    "![regularizaci√≥n](img/5.3.2_Regularization.png)\n",
    "\n",
    "Existen ~~dos~~ tres tipos de regularizaci√≥n: $L_2$ o Ridge, $L_1$ o Lasso, y Elastic net, la combinaci√≥n de ambas. Todas se aplican sobre la regresi√≥n lineal üòâ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regularization\n",
    "Esta regularizaci√≥n minimiza los coeficientes acerc√°ndolos a cero, pero nunca llegan a cero.\n",
    "Tenemos esta regularizaci√≥n en el objeto `Ridge`, es parecido a `LinearRegression` pero recibe el par√°metro `alpha` que va de 0 a 1, mientras m√°s alto sea, mayor ser√° la restricci√≥n sobre el modelo.\n",
    "\n",
    "Seguiremos el mismo procedimiento que antes: cargar los datos, seleccionar features, dividir en train y test para finalmente entrenar dos modelos y comparar sus coeficientes, pero esta vez entrenaremos sobre una peque√±a muestra de 20 datos, porque utilizar m√°s soluciona el overfitting üòÑ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "data_housing = pd.read_csv('./datasets/housing.csv')\n",
    "data_housing.dropna(inplace=True)\n",
    "\n",
    "X = data_housing['median_income']\n",
    "y = data_housing['median_house_value']\n",
    "\n",
    "# podemos tomar un n√∫mero espec√≠fico si pasamos un entero\n",
    "X_train, _, y_train, _ = train_test_split(X.values.reshape(-1, 1), y, train_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos tambi√©n Ridge\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# exageremos el grado para incrementar complejidad\n",
    "poly = PolynomialFeatures(degree=4, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "\n",
    "# tendremos 2 modelos para comparar\n",
    "model = LinearRegression()\n",
    "model_l2 = Ridge(alpha=1)\n",
    "\n",
    "model.fit(X_train_poly, y_train)\n",
    "model_l2.fit(X_train_poly, y_train)\n",
    "\n",
    "print(f\"Coeficientes Linear {model.coef_}\")\n",
    "print(f\"Coeficientes Ridge  {model_l2.coef_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ves, los coeficientes han sido reducidos, pero ninguno ha desaparecido.\n",
    "Esto modifica al modelo de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xpoints = np.linspace(min(X_train[:, 0]), max(X_train[:, 0]), 30)\n",
    "xpoints_poly = poly.transform(xpoints.reshape(-1, 1))\n",
    "ypoints = model.predict(xpoints_poly)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(X_train[:, 0], y_train, color=\"grey\")\n",
    "\n",
    "# el par√°rmetro color define el color de la linea\n",
    "plt.plot(xpoints, ypoints, color=\"red\", label=\"Linear\")\n",
    "plt.plot(xpoints, model_l2.predict(xpoints_poly), color=\"blue\", linewidth=\"2\", label=\"Ridge\")\n",
    "\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regularizaci√≥n ha restringido la curva, ya no se ajusta demasiado a los datos.\n",
    "El par√°metro `linewitdh` cambia el grosor, y `label` pone etiquetas a las gr√°ficas, pero para mostrarlas debemos llamar a `plt.legend` el par√°metro `loc` con el valor \"best\" tratar√° de determinar la mejor ubicaci√≥n para las etiquetas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regularization\n",
    "El objeto `Lasso` es id√©ntico a `Ridge` con la diferencia de que **s√≠ puede** reducir los coeficientes menos importantes a cero. Esto nos puede ayudar a decidir qu√© features son menos importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model_l1 = Lasso(alpha=1)\n",
    "model_l1.fit(X_train_poly, y_train)\n",
    "\n",
    "print(f\"Coeficientes Linear {model.coef_}\")\n",
    "print(f\"Coeficientes Lasso  {model_l1.coef_}\")\n",
    "\n",
    "plt.scatter(X_train[:, 0], y_train, color=\"grey\")\n",
    "\n",
    "# tambien puedes usar la letra inicial de ciertos colores\n",
    "plt.plot(xpoints, ypoints, color=\"r\", label=\"Linear\")\n",
    "plt.plot(xpoints, model_l1.predict(xpoints_poly), color=\"b\", linewidth=\"2\", label=\"Lasso\")\n",
    "\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La curva se ha aplanado m√°s, y en este caso una curva es de hecho m√°s adecuada que una recta, pero quiz√° bastar√≠a una de grado 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net\n",
    "Pretende equilibrar a Lasso combin√°ndolo con Ridge, e incluye el par√°metro `l1_ratio` que determina cu√°nta importancia, entre 0 y 1, se le dar√° a Lasso. Por tanto Ridge tendr√° el **complemento** de ese par√°metro, si `l1_ratio` es 0.3 entonces L2 tendr√° 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "model_elastic_net = ElasticNet(alpha=0.5, l1_ratio=0.4)\n",
    "model_elastic_net.fit(X_train_poly, y_train)\n",
    "\n",
    "print(f\"Coeficientes Ridge (L2)   {model_l2.coef_}\")\n",
    "print(f\"Coeficientes Lasso (L1)   {model_l1.coef_}\")\n",
    "print(f\"Coeficientes Elastic net  {model_elastic_net.coef_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net s√≥lo ha reducido un coeficiente a cero, de esta manera controlamos que no se eliminen varias features, por esto puede ser preferible usarla en lugar de Lasso, pero tenemos otro par√°metro que determinar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios\n",
    "Grafica la curva de Elastic Net, luego var√≠a el valor alfa de todos los modelos, ¬øQu√© ocurre con valores bajos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misma gr√°fica, diferente modelo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provoca deliberadamente (como en este cap√≠tulo) *overfitting* o *underfitting* con varias features, luego aplica las correcciones necesarias y eval√∫a el resultado con m√©tricas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revisa los dos cap√≠tulos anteriores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Par√°metros e Hiperpar√°metros\n",
    "Hasta ahora he llamado \"par√°metro\" a dos t√©rminos que son muy parecidos: los **coeficientes del modelo** y los **argumentos** de las funciones y objetos que usamos. Pero tienen una diferencia fundamental:\n",
    "\n",
    "La lista de coeficientes es **determinada por el modelo**, esto en *machine learning* se sigue conociendo como **par√°metro** üòÑ.  \n",
    "\n",
    "Aquello que **no determina el modelo** es un **hiperpar√°metro**, como las features elegidas, el porcentaje de divisi√≥n entre train y test, el grado de las features polinomiales, el coeficiente de regularizaci√≥n o el ratio entre $L_1$ y $L_2$ para Elastic Net.\n",
    "\n",
    "Aprenderemos m√°s hiperpar√°metros seg√∫n aprendamos m√°s conceptos, y uno de los **hiper**par√°metros m√°s importantes es el *learning rate* o **ratio de aprendizaje**, este controla qu√© tanto se ajusta un modelo por cada iteraci√≥n del algoritmo que permite a un modelo aprender üß†.\n",
    "\n",
    "Para la regresi√≥n lineal, el algoritmo es *Gradient Descent* o [Descenso del gradiente](6_Descenso_del_gradiente.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
