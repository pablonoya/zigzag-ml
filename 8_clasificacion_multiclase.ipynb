{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificaci√≥n Multiclase\n",
    "Tenemos **tres estrategias** para predecir a cu√°l de las especies de iris pertenece un ejemplo, aplicamos una a medias (m√°s bien a tercias) en el cap√≠tulo anterior, veamos cu√°l es."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zigzag Machine learning es una colecci√≥n de Jupyter Notebooks que pretende ense√±ar las bases del Machine Learning.\n",
    "Sigue un enfoque *top-down*, de arriba hacia abajo, por lo general y *bottom-up*, de abajo hacia arriba, cuando es necesario (o interesante). Por estas subidas y bajadas tenemos un zigzag üòÑ\n",
    "\n",
    "# Requisitos\n",
    "Puedes utilizar [Anaconda](https://www.anaconda.com/distribution/) que tiene todo lo necesario y algunos extras.  \n",
    "O [Miniconda](https://docs.conda.io/en/latest/miniconda.html) para instalar s√≥lo lo necesario.\n",
    "\n",
    "Cuando lo tengas configurado clona el repositorio, y dentro de √©l ejecuta:\n",
    "\n",
    "    conda env create -f env.yml\n",
    "    conda activate zzml\n",
    "    \n",
    "Finalmente abre Jupyter Lab\n",
    "\n",
    "    $ conda activate zzml\n",
    "\n",
    "O el cl√°sico Jupyter Notebook\n",
    "\n",
    "    $ conda activate zzml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs the Rest\n",
    "La idea es etiquetar **una** clase como **positiva** y **todas las dem√°s** como **negativas**, es **una contra el resto**. En el cap√≠tulo anterior distinguimos entre **setosa** y **no-setosa**, esto lo hacemos para todas las clases, entrenando un clasificador **distinto** cada caso.\n",
    "\n",
    "Esto no es muy complicado de implementar, pero por suerte `LogisticRegression` ya cuenta con la opci√≥n de usar dicha estrategia, estableciento su par√°metro `multi_class` en `'ovr'`.\n",
    "\n",
    "Este modelo espera que `y` sea un array, **no es necesario** que realicemos el one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "df = pd.read_csv(\"datasets/Iris.csv\")\n",
    "y = df[\"Species\"]\n",
    "\n",
    "# no necesitamos estas columnas\n",
    "# guardamos las dem√°s en X\n",
    "X = df.drop(columns=['Id', 'Species'])\n",
    "\n",
    "model_log = LogisticRegression(multi_class=\"ovr\")\n",
    "model_log.fit(X, y)\n",
    "\n",
    "# realicemos algunas predicciones\n",
    "X_sample = X.sample(5)\n",
    "model_log.predict(X_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambi√©n tenemos la implementaci√≥n en `OneVsRestClassifier` de `sklearn.multiclass`, este recibe una **instancia del modelo** que usaremos, por supuesto, debe ser uno de **clasificaci√≥n**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "model_ovr = OneVsRestClassifier( LogisticRegression() )\n",
    "model_ovr.fit(X, y)\n",
    "model_ovr.predict(X_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs One\n",
    "Entrenamos clasificadores por pares, **uno contra uno** distinguimos setosa de versicolor, setosa de virginica, versicolor de virginica, y as√≠ hasta entrenar todas las combinaciones, siendo $K (K-1) / 2 $ en total, donde $K$ es el n√∫mero de clases.\n",
    "\n",
    "La ventaja es que cada clasificador se entrena **s√≥lo con los datos que contienen las clases a distinguir**, algunos modelos **no escalan bien con el tama√±o del dataset**, es decir, rinden mal con un dataset m√°s grande.  \n",
    "En estos casos es m√°s r√°pido entrenar **muchos** clasificadores en **peque√±os** datasets que **pocos** en datasets m√°s **grandes**. \n",
    "\n",
    "Usaremos `OneVsOneClassifier` tambi√©n de `sklearn.multiclass` pues `LogisticRegression` no la soporta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "model_ovo = OneVsOneClassifier(LogisticRegression())\n",
    "model_ovo.fit(X, y)\n",
    "model_ovo.predict(___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresi√≥n log√≠stica multinomial\n",
    "Es una **generalizaci√≥n** de la regresi√≥n log√≠stica, que cubre **m√∫ltiples clases** directamente, eliminando la necesidad de entrenar varios clasificadores binarios.\n",
    "\n",
    "El par√°metro `multiclass` que vimos antes acepta los par√°metros `'auto'`, `'ovr'` y `'multinomial'`, **auto** es la opci√≥n por defecto, y esta selecciona **ovr** si `y` es **binario**.\n",
    "\n",
    "Como no es nuestro caso, seleccionar√° **multinomial**, por lo que ser√≠a opcional especificarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multi = LogisticRegression(multi_class='multinomial')\n",
    "model_multi.fit(X, y)\n",
    "model_multi.predict(X_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto tambi√©n es conocido como **Regresi√≥n Softmax**. Dado un ejemplo $x$, esta calcula primero un puntaje $s_k(x)$ para cada clase $k$\n",
    "\n",
    "$s_k(x) = x^T\\theta^{(k)}$\n",
    "\n",
    "Nota que **cada clase** tiene su propio **vector** $\\theta^{(k)}$. Cada uno se guarda en una fila de la matr√≠z $\\theta$ que est√° repartida en los atributos `coef_` e `intercept_` de nuestro modelo. Como vimos al implementar el [descenso del gradiente](5_descenso_del_gradiente.ipynb) $\\theta$ contiene los coeficientes y el t√©rmino independiente en una sola matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model_multi.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una columna por cada feature, una fila por cada clase üòâ\n",
    "\n",
    "Luego se aplica la funci√≥n **softmax** para calcular las probabilidades $\\hat{p}_k$ de que el ejemplo $x$ **pertenezca a una clase $k$** calculando una especie de media de las exponenciales, esto es $e$ elevado a los puntajes que antes calculamos.\n",
    "\n",
    "$\\hat{p}_k = \\dfrac{\\exp s_k(x)}{\\sum^K_{j=1}\\exp s_j(x)}$\n",
    "\n",
    "Donde $K$ es el n√∫mero total de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_multi.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al final, se predice la clase con la **probabilidad m√°s alta**, podemos usar `np.argmax` sobre un **vector** que contenga las **probabilidades**, esto nos dar√° el **√≠ndice** donde se encuentra el n√∫mero m√°s alto.\n",
    "\n",
    "Primero calculemos $s(x)$ como un vector que contega los **puntajes de cada clase**, por lo que tendr√° 3 elementos y ser√° el resultado de una multiplicaci√≥n de matrices, y no olvidemos sumar los t√©rminos independientes üòâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operamos con una matriz de numpy\n",
    "# y guardamos la primera fila\n",
    "X_sample1 = X_sample.to_numpy()[0]\n",
    "\n",
    "s = X_sample1 @ model_multi.coef_.T + model.intercept_\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego $p$ como otro vector de tres elementos que contiene las **probabilidades** de que nuestro ejemplo **pertenezca a alguna de las clases** 0, 1 o 2, setosa, versicolor o virginica, respectivamente üòÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.exp(s) / sum(np.exp(s))\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente obtenemos a qu√© clase pertenece nuestro ejemplo üòÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase de nuestro primer ejemplo\n",
    "pred = np.argmax(p)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es **versicolor**, como nuestros anteriores modelos lo predijeron üòÑ\n",
    "\n",
    "Ahora intenta predecir **todo** `X_sample`, al final `pred` deber√≠a ser un **array de 5 elementos** con cada una de las clases predichas üòâ  \n",
    "Adem√°s, `s` y `p` deber√≠an ser matrices de 3x5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada estrategia tiene su momento: **OvR** si el modelo es **estrictamente binario**, **OvO**  el modelo **no escala** bien con la **cantidad de datos** y la regresi√≥n **softmax** es como una regresi√≥n log√≠stica üòÑ (generalizada)\n",
    "\n",
    "Las primeras dos nos descubren que existen **m√°s modelos** de clasificaci√≥n, pero el pr√≥ximo que veremos usa softmax üòÇ y no por ello es menos interesante, es m√°s, te aseguro que su nombre te despierta inter√©s üß†:\n",
    "\n",
    "Te presento a las [redes neuronales artificiales](9_redes_neuronales.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
